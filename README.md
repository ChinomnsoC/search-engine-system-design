# System Design: Search Engine

## Overview: Scenario-Based DSA Problem (40 minutes)

This project implements a search engine that allows users to search for studies based on keywords. The search engine uses an inverted index to store and retrieve study titles that contain the given keywords.

ğŸ’¡ Scenario:
"You are building a feature for a research platform where users can search for past research studies they participated in. The search system must handle millions of queries efficiently. Each study has a unique study_id, a title, a category, and a description. Users should be able to:"

- Search studies by keyword (title or description)
- Get search results ranked by relevance (i.e., keyword frequency)
- Support autocomplete suggestions when users start typing

Your Task:

- Design a system to store and search studies efficiently.
- Choose the best data structure for fast search and ranking.
- Implement a function search_studies(keyword: str) -> List[str] that returns study titles matching the keyword.
- Optimize for fast lookups, considering scalability.

## Clarifications

I understand that this system is basically like a search engine. I have a couple of questions to clarify some things (how many searches per second do we want to have?, do we want to require the unique fields in search queries?). But I am mindful of the fact that we have 40 minutes, and I know that cannot be enough time to thoroughly complete all these tasks. Is there an aspect you'd like me to focus on first?

## ğŸ¯ Prioritizing the Problem

You're absolutely right! We donâ€™t have enough time to build everything, so letâ€™s focus on:

âœ… Step 1: Choosing the Right Data Structure for efficient search

âœ… Step 2: Implementing search_studies(keyword: str) -> List[str]

âœ… Step 3 (if time permits): Adding autocomplete functionality.

![Storage Options](data_structure.png)

Inverted Index is how search engines work. Trie is useful for autocomplete if we have time later.

## The Design

Since this system behaves like a search engine, we need a design that supports fast lookups, filtering, ranking, and scalability. Instead of just storing studies in a traditional database, we should optimize for search efficiency and retrieval speed.

### ğŸ¯ Step 1: Understanding the Requirements

ğŸ“Œ Before choosing a data store, letâ€™s break down our core functional needs:

- Keyword-Based Search â†’ Users should be able to search by title, description, or keywords.
- Filtering by Category â†’ Every study belongs to a unique category, so we need efficient category-based filtering.
- Autocomplete â†’ The system should provide real-time suggestions when users start typing.
- Scalability â†’ The search system should support millions of studies and handle high query loads.
- Ranking by Relevance â†’ More relevant results should appear higher in the search results.

ğŸ“Œ Given these requirements, we need to choose the right data store.

### ğŸ’¾ Step 2: Choosing the Right Storage Solution

ğŸ’¡ "We have two main options: a relational database (SQL) or a specialized search engine (NoSQL or full-text search database)."
![Storage Options](storage_options.png)

Given that we need a high-performance search system, we should use Elasticsearch as the primary search engine, supported by Redis for autocomplete. This setup provides fast full-text search, ranking, and real-time suggestions.

### ğŸ›  Step 3: Designing the Data Model

ğŸ’¡ "Now that weâ€™ve picked Elasticsearch, we need to define our document schema."

ğŸ“Œ Study Document Structure (Stored in Elasticsearch)

```json
{
  "study_id": 1,
  "title": "Machine Learning in Psychology",
  "description": "Understanding cognition using ML",
  "category": "Cognitive Science",
  "keywords": ["machine learning", "cognition", "psychology"],
  "timestamp": "2024-02-10T12:00:00Z"
}
```

âœ” Why this structure?

- The keywords field improves search relevance.
- The category field allows efficient filtering.
- The timestamp field enables sorting by recency.

ğŸ“Œ Indexing Strategy in Elasticsearch

- Title & description â†’ Full-text search
- Category â†’ Keyword-based filtering
- Timestamp â†’ Sorting by recency

### Step 4: Implementing Search Functionality and Step 5: Handling Autocomplete with Redis Trie

These are handled in the code file: [`search_studies.py`](search_studies.py)

## ğŸ’¡ Where Are We Using the Inverted Index?

âœ… Inverted Index is a key part of Elasticsearch's search mechanism.

An inverted index is how search engines quickly find documents that contain a specific keyword. Instead of scanning every study, we precompute a mapping of words to document locations, allowing us to retrieve results in near-instant time.

ğŸ“Œ How Elasticsearch Uses an Inverted Index:

1ï¸âƒ£ When a study is added â†’ Elasticsearch tokenizes its title, description, and keywords into separate words and builds an inverted index.

2ï¸âƒ£ When a user searches for a word â†’ Elasticsearch looks up the precomputed index instead of scanning all documents.

3ï¸âƒ£ Ranking happens next â†’ Using BM25 (TF-IDF variant), results are ranked by relevance.


## ğŸš€ How to Scale the Study Search System for Millions of Studies

Now that we have a search system based on Elasticsearch and Redis for autocomplete, letâ€™s discuss how to scale it efficiently.

1ï¸âƒ£ Scaling Data Storage & Indexing
ğŸ“Œ Problem:

Millions of studies mean our Elasticsearch index and Redis Trie will grow significantly.
If not optimized, indexing large datasets will be slow and queries will take longer.
ğŸ“Œ Solution:

âœ… Sharding in Elasticsearch â†’ Distribute the index across multiple nodes

âœ… Tiered Storage â†’ Keep frequently accessed studies in fast SSD storage

âœ… Bulk Indexing â†’ Process large updates efficiently

ğŸ’¡ Implementation Strategy:

Instead of one massive Elasticsearch index, we split it into shards based on category or time period.
Use hot-warm-cold architecture:
Hot tier â†’ Recent studies (fast SSD storage, high availability)
Warm tier â†’ Less frequent queries (stored on HDDs but still accessible)
Cold tier â†’ Archival studies (cheaper storage, long-term retention)
ğŸ“Œ Elasticsearch Sharding Example

```json
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 2
  }
}
```

ğŸ”¥ This ensures data is evenly distributed and improves query speed by parallelizing searches across nodes.

2ï¸âƒ£ Scaling Query Performance

ğŸ“Œ Problem:

As the dataset grows, full-text queries will become slower due to large index sizes.
The system should support low-latency queries (~50ms response time).

ğŸ“Œ Solution:

âœ… Precompute search rankings using BM25 scoring

âœ… Use query caching to reduce redundant searches

âœ… Optimize Elasticsearch mapping (disable unused fields, use lightweight indexing)

ğŸ’¡ Implementation Strategy:

Enable Elasticsearch query caching for frequently searched terms.
Use approximate nearest neighbor (ANN) search instead of full-text scan.
Apply query rewriting:
Instead of searching "machine learning cognition psychology," convert it to:

```json

{
  "query": {
    "bool": {
      "must": [
        { "match": { "title": "machine learning" }},
        { "match": { "description": "cognition psychology" }}
      ]
    }
  }
}
```

This makes search queries faster by breaking them into smaller, more relevant parts.
3ï¸âƒ£ Scaling Autocomplete with Redis
ğŸ“Œ Problem:

Autocomplete queries happen before the user submits a full search.
The Redis Trie structure will get very large with millions of entries.
ğŸ“Œ Solution:
âœ… Partition Redis by category to reduce lookup time
âœ… Evict old autocomplete suggestions that are rarely used
âœ… Use a compressed prefix tree to store data efficiently

ğŸ’¡ Implementation Strategy:

Instead of one giant Redis instance, store study titles based on category

```yaml

Redis Trie Partitions:
â”œâ”€â”€ Cognitive Science
â”œâ”€â”€ Machine Learning
â”œâ”€â”€ Behavioral Neuroscience
```
Use Redis LRU (Least Recently Used) eviction to remove unpopular suggestions.
4ï¸âƒ£ Scaling Traffic Handling
ğŸ“Œ Problem:

A surge in users (e.g., peak research periods) could overload the system.
API rate limiting is needed to prevent abuse.
ğŸ“Œ Solution:
âœ… Load Balancing (Nginx, AWS ALB) to distribute traffic
âœ… Rate Limiting using API Gateway (e.g., only 5 requests per second per user)
âœ… Circuit Breakers to stop queries if Elasticsearch/Redis slow down

ğŸ’¡ Implementation Strategy:

```json

{
  "rate_limit": {
    "requests_per_second": 5,
    "burst_limit": 10
  }
}
```

ğŸ”¥ This prevents any single user from overloading the system!

5ï¸âƒ£ Ensuring High Availability & Fault Tolerance
ğŸ“Œ Problem:

If an Elasticsearch node crashes, we canâ€™t afford downtime.
Failover & redundancy are critical.
ğŸ“Œ Solution:
âœ… Multi-region replication â†’ Store copies of indexes in different regions
âœ… Auto-healing clusters â†’ Use Kubernetes to restart failed services
âœ… Backup strategies â†’ Nightly backups of Elasticsearch & Redis

ğŸ’¡ Implementation Strategy:

```yaml

replicas: 3  # Run 3 Elasticsearch instances
antiAffinity: preferred  # Spread across multiple machines
```
ğŸ”¥ This ensures no single failure takes down the system!

### ğŸ¯ Final Scalable Architecture

```pgsql

           User Query
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                â”‚
  Full-Text Search    Autocomplete
 (Elasticsearch)      (Redis Trie)
      â”‚                â”‚
  Load Balanced     Partitioned Index
   API Gateway
      â”‚
  Sharded Indexing
(Elasticsearch Cluster)
      â”‚
  Multi-Region Replication
```
ğŸš€ Final Answer Summary
ğŸ“Œ To scale for millions of studies, we:
âœ… Use Elasticsearch sharding to distribute data across multiple nodes
âœ… Optimize query performance with caching & query rewriting
âœ… Partition Redis for autocomplete and use LRU eviction
âœ… Implement load balancing & rate limiting for API protection
âœ… Ensure fault tolerance with multi-region replication

ğŸ”¥ This keeps our system fast, scalable, and highly available!